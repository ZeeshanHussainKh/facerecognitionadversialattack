{"cells":[{"cell_type":"markdown","metadata":{"id":"UAwLb09GbLWv"},"source":["Lets Begin Coding\n","1. Download and Install required modules and libraries\n","2. Import required data and connect with drive\n"]},{"cell_type":"markdown","metadata":{"id":"KuAsH4gqBdcC"},"source":["\n","Importing all files and models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1eijdBjhzUCc","outputId":"e19d8a87-8b21-4552-93f8-32eee4b8c461","executionInfo":{"status":"ok","timestamp":1677529127442,"user_tz":-60,"elapsed":23579,"user":{"displayName":"zeeshan khand","userId":"17103399542843318646"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting lpips\n","  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.8/dist-packages (from lpips) (1.22.4)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from lpips) (1.7.3)\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from lpips) (1.13.1+cu116)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from lpips) (0.14.1+cu116)\n","Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.8/dist-packages (from lpips) (4.64.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.0->lpips) (4.5.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.2.1->lpips) (8.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.2.1->lpips) (2.25.1)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.2.1->lpips) (2.10)\n","Installing collected packages: lpips\n","Successfully installed lpips-0.1.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting focal-loss-torch\n","  Downloading focal_loss_torch-0.1.1-py3-none-any.whl (4.4 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from focal-loss-torch) (1.22.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from focal-loss-torch) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->focal-loss-torch) (4.5.0)\n","Installing collected packages: focal-loss-torch\n","Successfully installed focal-loss-torch-0.1.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting split-folders\n","  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n","Installing collected packages: split-folders\n","Successfully installed split-folders-0.5.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting PTable\n","  Downloading PTable-0.9.2.tar.gz (31 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: PTable\n","  Building wheel for PTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PTable: filename=PTable-0.9.2-py3-none-any.whl size=22925 sha256=a9ed4e2fed75321b4f615b2802594b612d43402be2b89e6ddd5a937486b22f8c\n","  Stored in directory: /root/.cache/pip/wheels/1b/3a/02/8d8da2bca2223dda2f827949c88b2d82dc85dccbc2bb6265e5\n","Successfully built PTable\n","Installing collected packages: PTable\n","Successfully installed PTable-0.9.2\n"]}],"source":["\n","!pip install lpips\n","!pip install torch torchvision\n","#!pip install mxnet-cu90 #optional, for data processing MXNet 1.3.1\n","!pip install focal-loss-torch\n","!pip install split-folders # Used for split folders into train and val datasets or in our case split data into gallery and probe\n","!pip install PTable\n","#https://torchmetrics.readthedocs.io/en/latest/image/learned_perceptual_image_patch_similarity.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSc74we_zeWk"},"outputs":[],"source":["import torch\n","from PIL import Image\n","import numpy as np\n","import lpips\n","import torchvision.transforms as transforms\n","import h5py\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","# used to supress display of warnings\n","import warnings\n","from sklearn.metrics import precision_recall_curve,accuracy_score,f1_score,precision_score,recall_score\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import os\n","\n","#from tensorboardX import SummaryWriter\n","from tqdm import tqdm\n","\n","#4\n","import cv2\n","import torchvision.datasets as datasets\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","\n","#5\n","import torch.utils.data as data\n","import torch.nn.functional as F\n","\n","#66\n","from easydict import EasyDict as edict\n","from pathlib import Path\n","from torch.nn import CrossEntropyLoss\n","from torchvision import transforms as trans\n","from torch.utils.data import Dataset, ConcatDataset, DataLoader\n","from torchvision import transforms as trans\n","from torchvision.datasets import ImageFolder\n","from PIL import Image, ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","import numpy as np\n","import cv2\n","import pickle\n","\n","\n","import splitfolders\n","from torch import nn, optim, as_tensor\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torch.optim import lr_scheduler\n","from torch.nn.init import *\n","from torchvision import transforms, utils, datasets, models\n","import cv2\n","from PIL import Image\n","from pdb import set_trace\n","import time\n","import copy\n","from pathlib import Path\n","import os\n","import sys\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from skimage import io, transform\n","from tqdm import trange, tqdm\n","import csv\n","import glob\n","import dlib\n","import pandas as pd\n","import numpy as np\n","from prettytable import PrettyTable\n","from google.colab import drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"F7xI6wdy-8-N","outputId":"ad6159ae-4f1a-4ecf-85a2-06a7bc753236","executionInfo":{"status":"error","timestamp":1677533157432,"user_tz":-60,"elapsed":125733,"user":{"displayName":"zeeshan khand","userId":"17103399542843318646"}}},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;34m': timeout during initial read of root folder; for more info: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             'https://research.google.com/colaboratory/faq.html#drive-timeout')\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed: timeout during initial read of root folder; for more info: https://research.google.com/colaboratory/faq.html#drive-timeout"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1l7xEBH9vu4H"},"outputs":[],"source":["%cd '/content/drive/MyDrive/Lowkey code/supp_material'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WtwO8e677uh4"},"outputs":[],"source":["%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4PbZtSX2-Q8I"},"outputs":[],"source":["#convert to tensor \n","to_tensor = transforms.ToTensor()"]},{"cell_type":"markdown","metadata":{"id":"Uw7DIssGcdbS"},"source":["A little info about folders\n","1. Align,util, models and backbone are imported from lowkey. Align is a copy of similar folder from FaceEvolve.\n","2. Face is a clone of complete repository of FaceEvolve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4JnQag1_Oo1q"},"outputs":[],"source":["import os\n","import sys\n","import math\n","import numbers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QPSfXEK2kY_"},"outputs":[],"source":["from util.prepare_utils import prepare_models,prepare_dir_vec,get_ensemble,extract_features # Used to load attack backbone in code\n","from util.feature_extraction_utils import normalize_transforms,feature_extractor\n","from util.attack_utils import Attack\n","from align.detector import detect_faces\n","from align.align_trans import get_reference_facial_points,warp_and_crop_face\n","from backbone.model_irse import IR_152\n","import lpips\n","from face.applications.align.visualization_utils import show_results\n","\n","#1\n","\n","from face.head.metrics import ArcFace,CosFace,SphereFace\n","\n","#2\n","from backbone.model_irse import IR_50, IR_101, IR_152, IR_SE_50, IR_SE_101, IR_SE_152\n","from backbone.model_resnet import ResNet_50, ResNet_101, ResNet_152\n","\n","from face.loss.focal import FocalLoss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9oHw3urciGc"},"outputs":[],"source":["if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NS9x6iju4jsU"},"outputs":[],"source":["device"]},{"cell_type":"markdown","metadata":{"id":"3bLLckiok18r"},"source":[" Parameters for lowkey adversial Attack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ho1wI39O-xci"},"outputs":[],"source":["# Parameters for lowkey adversial Attack\n","eps = 0.05\n","n_iters = 50 # No of epooch for the attack\n","input_size = [112, 112] # Input size model expects \n","attack_type = 'lpips' # attack type used\n","c_tv = None\n","c_sim = 0.05\n","lr = 0.0025 # the learning rate to optimize attack on\n","net_type = 'alex'\n","noise_size = 0.005 # noise size used\n","n_starts = 1\n","kernel_size_gf = 7 # gaussian smoothing window\n","sigma_gf = 3 # sigma for gaussain smoothing\n","combination = True # A switch to use gaussain smoothing together in attack or not\n","using_subspace = False\n","V_reduction_root = './'\n","direction = 1\n","crop_size = 112\n","scale = crop_size / 112 #1 \n"]},{"cell_type":"markdown","metadata":{"id":"WEA1GJDrdO2C"},"source":["Parameters for training by Face Evolve. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDSkFupCdKzA"},"outputs":[],"source":["\n","EMBEDDING_SIZE = 512 # feature dimension\n","BATCH_SIZE = 512\n","DROP_LAST = True # whether drop the last batch to ensure consistent batch_norm statistics\n","LR = 0.1 # initial LR\n","NUM_EPOCH = 120 # total epoch number (use the firt 1/25 epochs to warm up)\n","WEIGHT_DECAY = 5e-4 # do not apply to batch_norm parameters\n","MOMENTUM = 0.9\n","STAGES = [35, 65, 95] # epoch stages to decay learning rate\n","HEAD_NAME = 'ArcFace' # support:  ['Softmax', 'ArcFace', 'CosFace', 'SphereFace', 'Am_softmax']\n","LOSS_NAME = 'Focal' # support: ['Focal', 'Softmax']\n","\n","#INPUT_SIZE = [112, 112] # support: [112, 112] and [224, 224]\n","RGB_MEAN = [0.5, 0.5, 0.5] # for normalize inputs to [-1, 1]\n","       \n","#DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","MULTI_GPU = True # flag to use multiple GPUs; if you choose to train with single GPU, you should first run \"export CUDA_VISILE_DEVICES=device_id\" to specify the GPU card you want to use\n","GPU_ID = [0] # specify your GPU ids\n","PIN_MEMORY = True\n","NUM_WORKERS = 0\n","\n","RGB_MEAN = [0.5, 0.5, 0.5] # for normalize inputs to [-1, 1]\n","RGB_STD = [0.5, 0.5, 0.5]"]},{"cell_type":"markdown","metadata":{"id":"onvbmXgKph-D"},"source":["### 1. Load all the **Models**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6oBcaXD_6zNS"},"outputs":[],"source":["model_backbones = ['IR_152', 'IR_152', 'ResNet_152', 'ResNet_152'] # Model backbone for ensemble\n","model_roots = ['models/Backbone_IR_152_Arcface_Epoch_112.pth', 'models/Backbone_IR_152_Cosface_Epoch_70.pth', \\\n","'models/Backbone_ResNet_152_Arcface_Epoch_65.pth', 'models/Backbone_ResNet_152_Cosface_Epoch_68.pth'] # Roots of the models \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_P5p85NZo7Ij","outputId":"f082ef6f-6f4d-4a33-f2dd-c23d55c3c2e6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":22}],"source":["# Load an IR_152 Cosface model\n","PATH = 'models/Backbone_IR_152_Cosface_Epoch_70.pth'\n","IR_152C = IR_152(input_size=input_size)\n","IR_152C.load_state_dict(torch.load(PATH))\n","\n","# Load IR_152 Arc Face model\n","PATH1 = 'models/Backbone_IR_152_Arcface_Epoch_112.pth'\n","IR_152A = IR_152(input_size=input_size)\n","IR_152A.load_state_dict(torch.load(PATH1))\n","\n","#Load ResNet 152 Arc Face Model\n","PATH2 = 'models/Backbone_ResNet_152_Arcface_Epoch_65.pth'\n","RESNET_152A = ResNet_152(input_size=input_size)\n","RESNET_152A.load_state_dict(torch.load(PATH2))\n","\n","\n","# Load Resnet 152 CosFace Model\n","PATH3 = 'models/Backbone_ResNet_152_Cosface_Epoch_68.pth'\n","RESNET_152C = ResNet_152(input_size=input_size)\n","RESNET_152C.load_state_dict(torch.load(PATH3))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qy4ISChOAihv","outputId":"07562c7f-6903-4162-9bb1-bc52cc89fb43"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Attack Backbone Checkpoint '['models/Backbone_IR_152_Arcface_Epoch_112.pth', 'models/Backbone_IR_152_Cosface_Epoch_70.pth', 'models/Backbone_ResNet_152_Arcface_Epoch_65.pth', 'models/Backbone_ResNet_152_Cosface_Epoch_68.pth']'\n","====================\n"]}],"source":["# Generate lowkey ensemble attack model\n","models_attack, V_reduction, dim = prepare_models(model_backbones,\n","              input_size,\n","              model_roots,\n","              kernel_size_gf,\n","              sigma_gf,\n","              combination,\n","              using_subspace,\n","              V_reduction_root)"]},{"cell_type":"markdown","source":["##2. Split the whole face-scrub dataset into train and val \n"," then move them to original folders. Make gallery =50 with 20 identities protected by lowkey. \n"," "],"metadata":{"id":"dsQLyhYQljs-"}},{"cell_type":"code","source":[],"metadata":{"id":"jTTzjEr-l3Xu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPS7Gek2AwHw"},"source":["### 2. Now lets perform attacks on 5 identities\n","1. Ben Affleck \n","2. Adam Brody \n","3. Alfred Molina\n","4. Andy Garcia\n","5. Andy Serkis\n","\n","---\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"smWa8sWAIV0L"},"outputs":[],"source":["# Function to protect a single image\n","\n","def protect_image(image):\n","  #img = Image.fromarray(img)\n","  img = Image.open(image)\n","  #print(img)\n","  reference = get_reference_facial_points(default_square=True) * scale\n","  print(\"The value for reference in get reference facial points  is {}\".format(reference))\n","  h,w,c = np.array(img).shape\n","  print (h,w,c)\n","\n","  b_box,landmarks = detect_faces(img) \n","  #show_results(img, _, landmarks)\n","  print(\"The size of landmarks is {}\".format(landmarks.size))  \n","  #print(\"The value for _ in detect faces is {}\".format(_))\n","  # (landmarks)\n","  facial5points = [[landmarks[0][j], landmarks[0][j + 5]] for j in range(5)]\n","  #print(\"The value for _ in detect faces is {} end \".format(facial5points))\n","  b_box, tfm = warp_and_crop_face(np.array(img), facial5points, reference, crop_size=(crop_size, crop_size))\n","  print(\"The value for tfm in warp_and_crop_face is {}  \".format(tfm))\n","  # pytorch transform\n","  theta = normalize_transforms(tfm, w, h)\n","  tensor_img = to_tensor(img).unsqueeze(0).to(device)\n","  print(\"The value for theta in normalize_transforms(tfm, w, h) is {}  \".format(theta))\n","  V_reduction = None\n","  dim = 512\n","\n","  # Find gradient direction vector\n","  dir_vec_extractor = get_ensemble(models = models_attack, sigma_gf=None, kernel_size_gf=None, combination=False, V_reduction=V_reduction, warp=True, theta_warp=theta)\n","  dir_vec = prepare_dir_vec(dir_vec_extractor, tensor_img, dim, combination)\n","  print(\"The value for direction vector of gradient in image is {}  \".format(dir_vec))\n","  img_attacked = tensor_img.clone()\n","  attack = Attack(models_attack, dim, attack_type, \n","                  eps, c_sim, net_type, lr, n_iters, \n","                  noise_size, n_starts, c_tv, sigma_gf, \n","                  kernel_size_gf, combination, warp=True, \n","                  theta_warp=theta, V_reduction=V_reduction)\n","  img_attacked = attack.execute(tensor_img, dir_vec, direction).detach().cpu()\n","\n","  img_attacked_pil = transforms.ToPILImage()(img_attacked[0])\n","  return img_attacked_pil,dir_vec_extractor\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVEecnxCIb2M"},"outputs":[],"source":["#Now lets create a function to attack a directory of images  with lowkey attack\n","\n","def protect_dir(dirs_root,model):\n","  \n","  for img_name in tqdm(os.listdir(dirs_root)):\n","\n","    \n","    img_root = os.path.join(dirs_root, img_name)\n","    #new_img_root = os.path.join(save_dir, img_name)\n","    print(\"The image processed is : {}\".format(img_root))\n","    print('Finding reference points')\n","    reference = get_reference_facial_points(default_square=True) * scale\n","    img = Image.open(img_root)\n","    #h,w,c = np.array(img).shape\n","    h,w,c = np.array(img).shape\n","    #print(\"The value for reference in get reference facial points  is {}\".format(reference))\n","    #print (h,w,c)\n","    ## Detects facial points ##\n","    b_box,landmarks = detect_faces(img) \n","    facial5points = [[landmarks[0][j], landmarks[0][j + 5]] for j in range(5)]\n","    #print(\"The value for _ in detect faces is {} end \".format(facial5points))\n","    b_box, tfm = warp_and_crop_face(np.array(img), facial5points, reference, crop_size=(crop_size, crop_size))\n","    #print(\"The value for tfm in warp_and_crop_face is {}  \".format(tfm))\n","    \n","    ## find pytorch transform\n","    theta = normalize_transforms(tfm, w, h)\n","    tensor_img = to_tensor(img).unsqueeze(0).to(device)\n","    #print(\"The value for theta in normalize_transforms(tfm, w, h) is {}  \".format(theta))\n","    \n","    V_reduction = None\n","    dim = 512\n","\n","  # Find gradient direction vector i.e find the feature vector extractor\n","    dir_vec_extractor = get_ensemble(models=model, sigma_gf=None, kernel_size_gf=None, combination=False, V_reduction=V_reduction, warp=True, theta_warp=theta)\n","    dir_vec = prepare_dir_vec(dir_vec_extractor, tensor_img, dim, combination).to(device)\n","  #print(\"The value for direction vector of gradient in image is {}  \".format(dir_vec))\n","  \n","  # Perform Low key attack\n","    img_attacked = tensor_img.clone()\n","    attack = Attack(model, dim, attack_type, \n","                  eps, c_sim, net_type, lr, n_iters, \n","                  noise_size, n_starts, c_tv, sigma_gf, \n","                  kernel_size_gf, combination, warp=True, \n","                  theta_warp=theta, V_reduction=V_reduction).to(device)\n","    img_attacked = attack.execute(tensor_img, dir_vec, direction).detach().cpu()\n","    print(\"attack Executed\")\n","    img_attacked_pil = transforms.ToPILImage()(img_attacked[0])\n","    img_attacked_pil.save(img_root[:-4] + '.png')\n","    os.remove(img_root)\n"]},{"cell_type":"markdown","metadata":{"id":"DMTnSKQgCKvZ"},"source":["### 3.Lowkey ensemble attack on the 5 identities"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aiK5jGvztOdV","outputId":"36affe0a-8409-4486-9bce-7d3ba79a8296"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/58 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5918_3439.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  2%|▏         | 1/58 [00:36<34:26, 36.26s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5896_3424.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  3%|▎         | 2/58 [01:13<34:32, 37.00s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5956_3451.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  5%|▌         | 3/58 [01:51<34:04, 37.18s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5908_3432.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  7%|▋         | 4/58 [02:27<33:06, 36.80s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5875_3412.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  9%|▊         | 5/58 [03:03<32:24, 36.70s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5914_3436.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 10%|█         | 6/58 [03:39<31:27, 36.31s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5904_3430.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 7/58 [04:16<30:58, 36.44s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5911_3435.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 8/58 [04:52<30:16, 36.34s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5871_3411.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 9/58 [05:29<29:47, 36.48s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5974_3459.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 17%|█▋        | 10/58 [06:06<29:28, 36.84s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5793_3359.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 19%|█▉        | 11/58 [06:43<28:48, 36.78s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5935_3444.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 21%|██        | 12/58 [07:21<28:26, 37.11s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5880_3415.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 13/58 [07:58<27:52, 37.17s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5903_3429.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 14/58 [08:35<27:15, 37.16s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5844_3393.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 15/58 [09:12<26:39, 37.20s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5700_3317.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 16/58 [09:50<26:11, 37.42s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5726_3329.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 29%|██▉       | 17/58 [10:28<25:35, 37.46s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5936_3445.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 31%|███       | 18/58 [11:06<25:00, 37.52s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5751_3340.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 33%|███▎      | 19/58 [11:43<24:23, 37.52s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5740_3336.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 20/58 [12:22<24:01, 37.92s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5762_3345.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 21/58 [12:59<23:14, 37.70s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5869_3410.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 22/58 [13:36<22:32, 37.56s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5737_3334.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 40%|███▉      | 23/58 [14:14<21:54, 37.56s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5725_3328.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 41%|████▏     | 24/58 [14:51<21:15, 37.53s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5782_3354.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 43%|████▎     | 25/58 [15:28<20:32, 37.34s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5738_3335.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 45%|████▍     | 26/58 [16:06<19:58, 37.46s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5689_3311.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 47%|████▋     | 27/58 [16:44<19:23, 37.53s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5790_3356.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 28/58 [17:20<18:35, 37.18s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5664_3295.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 29/58 [17:57<17:55, 37.07s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5701_3318.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 30/58 [18:35<17:24, 37.29s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5720_3327.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 53%|█████▎    | 31/58 [19:13<16:51, 37.45s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5772_3349.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 55%|█████▌    | 32/58 [19:49<16:06, 37.19s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5707_3321.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 57%|█████▋    | 33/58 [20:27<15:33, 37.33s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5792_3358.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 59%|█████▊    | 34/58 [21:05<14:59, 37.48s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5813_3374.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 35/58 [21:41<14:16, 37.25s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5688_3310.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 36/58 [22:19<13:39, 37.23s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5697_3315.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 37/58 [22:57<13:09, 37.58s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5657_3291.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 38/58 [23:34<12:27, 37.37s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5640_3278.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 67%|██████▋   | 39/58 [24:11<11:46, 37.21s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5656_3290.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 69%|██████▉   | 40/58 [24:48<11:11, 37.31s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5635_3274.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 71%|███████   | 41/58 [25:25<10:31, 37.15s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5651_3286.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 42/58 [26:02<09:51, 36.98s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5639_3277.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 43/58 [26:38<09:13, 36.88s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5666_3296.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 44/58 [27:17<08:44, 37.44s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5685_3308.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 45/58 [27:54<08:04, 37.26s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5634_3273.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 79%|███████▉  | 46/58 [28:30<07:24, 37.07s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5677_3304.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 81%|████████  | 47/58 [29:08<06:50, 37.31s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5648_3283.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 83%|████████▎ | 48/58 [29:46<06:13, 37.37s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5641_3279.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 49/58 [30:23<05:34, 37.18s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5652_3287.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 50/58 [31:00<04:58, 37.25s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5918_3439.protected.png\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 51/58 [31:37<04:19, 37.09s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5896_3424.protected.png\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 90%|████████▉ | 52/58 [32:13<03:41, 36.84s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5956_3451.protected.png\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 91%|█████████▏| 53/58 [32:49<03:02, 36.53s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5908_3432.protected.png\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 93%|█████████▎| 54/58 [33:26<02:27, 36.88s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5875_3412.protected.png\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 95%|█████████▍| 55/58 [34:02<01:49, 36.47s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5914_3436.protected.png\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 97%|█████████▋| 56/58 [34:38<01:12, 36.34s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5904_3430.protected.png\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 57/58 [35:15<00:36, 36.47s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Andy_Serkis/Andy_Serkis_5911_3435.protected.png\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 58/58 [35:51<00:00, 37.10s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# protect_dir(dirs_root=lowkey_id_3,model=models_attack)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["abded15c77b64851b2433c29a9c85ac6","b865646675444ff5bd920db1101524ed","c45a4d624c6944b083a1d99a9812a953","aa18884468404ab2930318ab12f60f89","031acb9ee2af4878a1508ee1cd1ff946","dfe08893fafe47a68f9ef1232e393a06","92682188a8c143698fe99486dc52bd00","4cb15a84bdba4d0fb417c9f8772ccfa6","739cc4ed8e2a4147af64d845da8b191d","ec9f5b75d95441efbc1c4ef919e05feb","891af9ef28404005810b6374e80ea819"]},"id":"0DIlBzCCtPYc","outputId":"34213036-c35a-4264-802b-feee0078f2ea"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/27 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8549_4727.jpeg\n","Finding reference points\n"]},{"name":"stderr","output_type":"stream","text":["/content/drive/MyDrive/Lowkey code/supp_material/align/first_stage.py:32: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  img = Variable(torch.FloatTensor(_preprocess(img)), volatile = True)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/get_nets.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  a = F.softmax(a)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/detector.py:81: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  img_boxes = Variable(torch.FloatTensor(img_boxes), volatile = True)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/get_nets.py:115: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  a = F.softmax(a)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/detector.py:102: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  img_boxes = Variable(torch.FloatTensor(img_boxes), volatile = True)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/get_nets.py:168: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  a = F.softmax(a)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/matlab_cp2tform.py:84: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n","To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n","  r, _, _, _ = lstsq(X, U)\n","/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"abded15c77b64851b2433c29a9c85ac6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/233M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  4%|▎         | 1/27 [00:51<22:28, 51.86s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8779_4882.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  7%|▋         | 2/27 [01:29<18:04, 43.38s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8681_4815.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 11%|█         | 3/27 [02:07<16:27, 41.13s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8572_4742.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 15%|█▍        | 4/27 [02:43<15:00, 39.16s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8518_4699.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 19%|█▊        | 5/27 [03:20<13:58, 38.11s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8479_4675.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 6/27 [03:56<13:05, 37.42s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8539_4718.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 7/27 [04:32<12:20, 37.00s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8486_4681.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 30%|██▉       | 8/27 [05:11<11:55, 37.65s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8586_4750.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 33%|███▎      | 9/27 [05:47<11:08, 37.13s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8525_4705.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 37%|███▋      | 10/27 [06:23<10:25, 36.80s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8617_4771.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 41%|████      | 11/27 [06:59<09:44, 36.54s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8480_4676.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 12/27 [07:37<09:15, 37.03s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8557_4733.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 13/27 [08:13<08:34, 36.72s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8519_4700.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 14/27 [08:49<07:55, 36.55s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8496_4684.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 15/27 [09:25<07:16, 36.39s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8526_4706.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 59%|█████▉    | 16/27 [10:03<06:46, 36.92s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8462_4663.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 63%|██████▎   | 17/27 [10:39<06:05, 36.58s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8607_4765.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 67%|██████▋   | 18/27 [11:15<05:27, 36.38s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8456_4657.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 19/27 [11:51<04:49, 36.13s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8603_4762.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 20/27 [12:26<04:11, 35.94s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8556_4732.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 21/27 [13:03<03:38, 36.35s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8477_4673.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 81%|████████▏ | 22/27 [13:39<03:00, 36.07s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8563_4737.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 85%|████████▌ | 23/27 [14:14<02:23, 35.90s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8471_4670.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 89%|████████▉ | 24/27 [14:50<01:47, 35.76s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8455_4656.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 93%|█████████▎| 25/27 [15:27<01:12, 36.31s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8461_4662.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 96%|█████████▋| 26/27 [16:03<00:36, 36.09s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_galleries/adversial_gallery_ensemble/Ben_Affleck/Ben_Affleck_8476_4672.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 27/27 [16:38<00:00, 37.00s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# protect_dir(dirs_root=lowkey_id_4,model=models_attack)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ea37fcac34ba42afb6601f15d9377720","9db6427757a54ba1b7120e74d0bf7a38","7d133229b0c847d99ad166ef68eac7e0","f1c3fe0030134ff38fec441b2711f059","b8d2c5a63cb141f5ab70602030a5ea61","75bb189558bc4aacaf8023e03d8eb3e6","f6af29233c3f4b7480b2d5433ac136b1","079b4a6168374c1f96eb0132d6448ca0","88f90b6935b84511b0db1331717916c5","8cdb9eab9d1548feb3d831b697aa3756","d521b57d1d944116adaddc025b5dbf6c"]},"id":"yd9VW0uctQX7","outputId":"1b5a7351-998d-4d58-8b8b-92d576514666"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/66 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4617_2585.jpeg\n","Finding reference points\n"]},{"name":"stderr","output_type":"stream","text":["/content/drive/MyDrive/Lowkey code/supp_material/align/first_stage.py:32: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  img = Variable(torch.FloatTensor(_preprocess(img)), volatile = True)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/get_nets.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  a = F.softmax(a)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/detector.py:81: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  img_boxes = Variable(torch.FloatTensor(img_boxes), volatile = True)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/get_nets.py:115: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  a = F.softmax(a)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/detector.py:102: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  img_boxes = Variable(torch.FloatTensor(img_boxes), volatile = True)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/get_nets.py:168: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  a = F.softmax(a)\n","/content/drive/MyDrive/Lowkey code/supp_material/align/matlab_cp2tform.py:84: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n","To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n","  r, _, _, _ = lstsq(X, U)\n","/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea37fcac34ba42afb6601f15d9377720","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/233M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  2%|▏         | 1/66 [00:49<54:05, 49.92s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4615_2583.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  3%|▎         | 2/66 [01:26<44:58, 42.17s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4614_2582.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  5%|▍         | 3/66 [02:02<41:08, 39.19s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4618_2586.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 4/66 [02:37<38:54, 37.65s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4632_2598.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 5/66 [03:14<38:05, 37.46s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4622_2588.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r  9%|▉         | 6/66 [03:50<36:56, 36.94s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4623_2589.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 11%|█         | 7/66 [04:27<36:10, 36.79s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4625_2591.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 8/66 [05:05<36:04, 37.32s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4631_2597.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 14%|█▎        | 9/66 [05:42<35:23, 37.26s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4629_2595.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 15%|█▌        | 10/66 [06:18<34:22, 36.84s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4653_2611.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 17%|█▋        | 11/66 [06:55<33:46, 36.85s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4626_2592.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 12/66 [07:31<32:48, 36.45s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4652_2610.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 20%|█▉        | 13/66 [08:05<31:47, 35.99s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4651_2609.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 21%|██        | 14/66 [08:42<31:26, 36.27s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4654_2612.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 23%|██▎       | 15/66 [09:17<30:30, 35.89s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4634_2599.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 16/66 [09:54<30:12, 36.24s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4662_2619.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 17/66 [10:29<29:17, 35.87s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4650_2608.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 27%|██▋       | 18/66 [11:05<28:33, 35.71s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4655_2613.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 29%|██▉       | 19/66 [11:40<27:55, 35.65s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4664_2621.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 30%|███       | 20/66 [12:15<27:10, 35.45s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4656_2614.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 21/66 [12:50<26:25, 35.23s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4674_2628.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 33%|███▎      | 22/66 [13:25<25:43, 35.08s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4675_2629.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 35%|███▍      | 23/66 [14:00<25:06, 35.03s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4676_2630.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 36%|███▋      | 24/66 [14:35<24:37, 35.18s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4672_2626.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 25/66 [15:10<23:58, 35.09s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4704_2644.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 39%|███▉      | 26/66 [15:45<23:18, 34.95s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4684_2637.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 41%|████      | 27/66 [16:20<22:42, 34.93s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4707_2646.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 28/66 [16:54<21:59, 34.73s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4738_2667.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 29/66 [17:29<21:27, 34.79s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4718_2655.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 45%|████▌     | 30/66 [18:05<21:03, 35.08s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4713_2651.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 47%|████▋     | 31/66 [18:39<20:20, 34.86s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4698_2643.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 32/66 [19:14<19:45, 34.86s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4728_2662.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 33/66 [19:48<19:06, 34.76s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4714_2652.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 34/66 [20:24<18:43, 35.10s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4720_2656.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 53%|█████▎    | 35/66 [21:00<18:13, 35.28s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4690_2640.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 55%|█████▍    | 36/66 [21:35<17:34, 35.16s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4723_2658.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 37/66 [22:09<16:54, 34.99s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4754_2677.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 38/66 [22:44<16:19, 35.00s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4741_2668.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 59%|█████▉    | 39/66 [23:19<15:45, 35.02s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4750_2674.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 61%|██████    | 40/66 [23:56<15:20, 35.40s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4786_2695.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 41/66 [24:31<14:42, 35.28s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4791_2699.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 64%|██████▎   | 42/66 [25:06<14:05, 35.21s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4799_2706.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 65%|██████▌   | 43/66 [25:40<13:25, 35.04s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4783_2693.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 67%|██████▋   | 44/66 [26:15<12:48, 34.94s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4819_2716.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 45/66 [26:51<12:22, 35.38s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4812_2713.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 70%|██████▉   | 46/66 [27:26<11:44, 35.22s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4797_2704.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 71%|███████   | 47/66 [28:01<11:07, 35.11s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4829_2721.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 73%|███████▎  | 48/66 [28:36<10:29, 34.98s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4833_2723.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 49/66 [29:11<09:53, 34.90s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4827_2720.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 50/66 [29:46<09:22, 35.17s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4867_2737.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 77%|███████▋  | 51/66 [30:21<08:46, 35.13s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4872_2740.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 79%|███████▉  | 52/66 [30:56<08:09, 34.94s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4881_2747.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 53/66 [31:31<07:33, 34.91s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4863_2735.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 54/66 [32:05<06:57, 34.80s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4826_2719.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 83%|████████▎ | 55/66 [32:41<06:24, 34.94s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4856_2732.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 85%|████████▍ | 56/66 [33:16<05:52, 35.22s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4887_2752.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 86%|████████▋ | 57/66 [33:51<05:15, 35.02s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4879_2746.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 58/66 [34:26<04:39, 34.93s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4898_2757.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 89%|████████▉ | 59/66 [35:00<04:04, 34.88s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4886_2751.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 91%|█████████ | 60/66 [35:35<03:29, 34.88s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4913_2766.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 61/66 [36:12<02:56, 35.40s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4928_2776.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 62/66 [36:46<02:20, 35.14s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4929_2777.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 95%|█████████▌| 63/66 [37:21<01:44, 34.97s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4930_2778.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 97%|█████████▋| 64/66 [37:56<01:09, 34.83s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4914_2767.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 65/66 [38:31<00:34, 34.90s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n","The image processed is : /content/drive/MyDrive/Lowkey code/faceScrub/actors/cleanData/dataAlign/adversial_data/Ensemble_attack/gallery/4.Amaury_Nolasco/Amaury_Nolasco_4956_2788.jpeg\n","Finding reference points\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 66/66 [39:07<00:00, 35.57s/it]"]},{"name":"stdout","output_type":"stream","text":["attack Executed\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["protect_dir(dirs_root=lowkey_id_5,model=models_attack)\n"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"031acb9ee2af4878a1508ee1cd1ff946":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"079b4a6168374c1f96eb0132d6448ca0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cb15a84bdba4d0fb417c9f8772ccfa6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"739cc4ed8e2a4147af64d845da8b191d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75bb189558bc4aacaf8023e03d8eb3e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d133229b0c847d99ad166ef68eac7e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_079b4a6168374c1f96eb0132d6448ca0","max":244408911,"min":0,"orientation":"horizontal","style":"IPY_MODEL_88f90b6935b84511b0db1331717916c5","value":244408911}},"88f90b6935b84511b0db1331717916c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"891af9ef28404005810b6374e80ea819":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8cdb9eab9d1548feb3d831b697aa3756":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92682188a8c143698fe99486dc52bd00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9db6427757a54ba1b7120e74d0bf7a38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75bb189558bc4aacaf8023e03d8eb3e6","placeholder":"​","style":"IPY_MODEL_f6af29233c3f4b7480b2d5433ac136b1","value":"100%"}},"aa18884468404ab2930318ab12f60f89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec9f5b75d95441efbc1c4ef919e05feb","placeholder":"​","style":"IPY_MODEL_891af9ef28404005810b6374e80ea819","value":" 233M/233M [00:04&lt;00:00, 76.1MB/s]"}},"abded15c77b64851b2433c29a9c85ac6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b865646675444ff5bd920db1101524ed","IPY_MODEL_c45a4d624c6944b083a1d99a9812a953","IPY_MODEL_aa18884468404ab2930318ab12f60f89"],"layout":"IPY_MODEL_031acb9ee2af4878a1508ee1cd1ff946"}},"b865646675444ff5bd920db1101524ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfe08893fafe47a68f9ef1232e393a06","placeholder":"​","style":"IPY_MODEL_92682188a8c143698fe99486dc52bd00","value":"100%"}},"b8d2c5a63cb141f5ab70602030a5ea61":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c45a4d624c6944b083a1d99a9812a953":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb15a84bdba4d0fb417c9f8772ccfa6","max":244408911,"min":0,"orientation":"horizontal","style":"IPY_MODEL_739cc4ed8e2a4147af64d845da8b191d","value":244408911}},"d521b57d1d944116adaddc025b5dbf6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfe08893fafe47a68f9ef1232e393a06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea37fcac34ba42afb6601f15d9377720":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9db6427757a54ba1b7120e74d0bf7a38","IPY_MODEL_7d133229b0c847d99ad166ef68eac7e0","IPY_MODEL_f1c3fe0030134ff38fec441b2711f059"],"layout":"IPY_MODEL_b8d2c5a63cb141f5ab70602030a5ea61"}},"ec9f5b75d95441efbc1c4ef919e05feb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1c3fe0030134ff38fec441b2711f059":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cdb9eab9d1548feb3d831b697aa3756","placeholder":"​","style":"IPY_MODEL_d521b57d1d944116adaddc025b5dbf6c","value":" 233M/233M [00:01&lt;00:00, 255MB/s]"}},"f6af29233c3f4b7480b2d5433ac136b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}